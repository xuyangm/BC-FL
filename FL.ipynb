{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNj8W1MEgiiLt1a/CITzm9H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xuyangm/BC-FL/blob/master/FL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms, datasets\n",
        "import numpy as np\n",
        "from torch.utils.data import sampler\n",
        "from itertools import combinations\n",
        "import random\n",
        "import math\n",
        "import copy\n",
        "\n",
        "class ClientInfo(object):\n",
        "    \"\"\"\n",
        "    Record some information related to a client\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, client_id):\n",
        "        self.client_id = client_id\n",
        "        self.last_involved_round = 0\n",
        "        self.contribution = 0.0\n",
        "        self.loss = 0.0\n",
        "        self.times = 0\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "data_train = datasets.CIFAR10('./data/cifar/', train=True, download=True, transform=train_transform)\n",
        "data_test = datasets.CIFAR10('../data/cifar/', train=False, download=True, transform=test_transform)\n",
        "\n",
        "\n",
        "def dirichlet_partition(data, num_clients, alpha):\n",
        "        \"\"\"Partition by Dirichlet distribution (non-iid)\"\"\"\n",
        "        labels = torch.as_tensor(data.targets)\n",
        "        num_classes = labels.max() + 1\n",
        "        label_distribution = np.random.dirichlet([alpha] * num_clients, num_classes)\n",
        "        class_idx = [np.argwhere(labels == y).flatten() for y in range(num_classes)]\n",
        "        client_idx = [[] for _ in range(num_clients)]\n",
        "\n",
        "        for c, fracs in zip(class_idx, label_distribution):\n",
        "            for i, idx in enumerate(np.split(c, ((np.cumsum(fracs)[:-1]) * len(c)).astype(int))):\n",
        "                client_idx[i] += [idx]\n",
        "\n",
        "        partitions = [np.concatenate(idx) for idx in client_idx]\n",
        "        return partitions\n",
        "\n",
        "def select_participants(explored, unexplored, sample_sz, explore_ratio, rd, clients_info):\n",
        "  participants = []\n",
        "  explore_num = min(len(unexplored), round(explore_ratio * sample_sz))\n",
        "  exploit_num = sample_sz - explore_num\n",
        "\n",
        "  if len(explored) < exploit_num:\n",
        "    participants = random.sample(unexplored, sample_sz)\n",
        "  else:\n",
        "    utility = {}\n",
        "    for cid in explored:\n",
        "      L = clients_info[cid].last_involved_round\n",
        "      utility[cid] = clients_info[cid].contribution + math.sqrt(0.1 * math.log(rd, 10) / L)\n",
        "\n",
        "    unexplored_participants = random.sample(unexplored, explore_num)\n",
        "    explored_participants = sorted(utility, key=utility.get, reverse=True)[:exploit_num]\n",
        "    participants = unexplored_participants + explored_participants\n",
        "  return participants\n",
        "\n",
        "def cal_shapley_values(loader, local_updates, accuracy, batch_size):\n",
        "    value_dict = {}\n",
        "    accuracy_dict = {}\n",
        "    client_ids = sorted(list(local_updates.keys()))\n",
        "\n",
        "    for i in range(len(local_updates)):\n",
        "        cmbs = list(combinations(client_ids, i+1))\n",
        "        for cmb in cmbs:\n",
        "            model = average_model(local_updates, cmb)\n",
        "            accuracy_dict[cmb], _ = test(model, loader)\n",
        "\n",
        "    for k in local_updates:\n",
        "        non_k = [x for x in client_ids if x != k]\n",
        "        counter = 1\n",
        "        value = accuracy_dict[(k,)] - accuracy\n",
        "\n",
        "        for j in range(1, len(local_updates)):\n",
        "            partner_list = combinations(non_k, j)\n",
        "            for partners in partner_list:\n",
        "                full_tup = tuple(sorted(partners + (k,)))\n",
        "                value = value + accuracy_dict[full_tup] - accuracy_dict[partners]\n",
        "                counter += 1\n",
        "\n",
        "        value_dict[k] = float(value / counter)\n",
        "\n",
        "    return value_dict\n",
        "\n",
        "partitions = dirichlet_partition(data_train, 100, 0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjGKnPaFvfVC",
        "outputId": "9a96f903-b7eb-458a-b560-c816eba66264"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from collections import OrderedDict\n",
        "\n",
        "def train(model, loader, lr, momentum, weight_decay, epoch):\n",
        "  model = model.cuda()\n",
        "\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "  criterion = torch.nn.CrossEntropyLoss().cuda()\n",
        "\n",
        "  step = 0\n",
        "\n",
        "  while step < epoch:\n",
        "    model.train()\n",
        "    step += 1\n",
        "    for (X, y) in loader:\n",
        "      X = X.cuda()\n",
        "      y = y.cuda()\n",
        "      output = model(X)\n",
        "      loss = criterion(output, y)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "  return model.state_dict()\n",
        "\n",
        "def test(model, loader):\n",
        "  model = model.cuda()\n",
        "  criterion = torch.nn.CrossEntropyLoss().cuda()\n",
        "  model.eval()\n",
        "  total_accuracy = total_loss = 0\n",
        "  for (X, y) in loader:\n",
        "    X = X.cuda()\n",
        "    y = y.cuda()\n",
        "    output = model(X)\n",
        "    total_accuracy += (output.argmax(1) == y).sum()\n",
        "    loss = criterion(output, y)\n",
        "    total_loss += loss.item()\n",
        "\n",
        "  # writer = SummaryWriter(\"logs/cifar10\")\n",
        "  # writer.add_scalar(\"loss\", total_loss/len(data_test), rd)\n",
        "  # writer.add_scalar(\"accuracy(%)\", total_accuracy/len(data_test)*100, rd)\n",
        "  # writer.close()\n",
        "  return total_accuracy/len(data_test), total_loss/len(data_test)\n",
        "                 \n",
        "def average_model(local_updates, cmb):\n",
        "  model = torchvision.models.shufflenet_v2_x2_0(num_classes=10)\n",
        "  sz = len(cmb)\n",
        "  if sz == 1:\n",
        "    model.load_state_dict(local_updates[cmb[0]])\n",
        "    return model\n",
        "\n",
        "  averaged_weights = OrderedDict()\n",
        "  for it, idx in enumerate(cmb):\n",
        "    local_weights = local_updates[idx]\n",
        "    for key in local_weights.keys():\n",
        "      if it == 0:\n",
        "        averaged_weights[key] = torch.div(local_weights[key], sz)\n",
        "      else:\n",
        "        averaged_weights[key] += torch.div(local_weights[key], sz)\n",
        "\n",
        "  model.load_state_dict(averaged_weights)\n",
        "  return model\n",
        "\n",
        "def random_client_sampler(clients, sample_sz):\n",
        "  return random.sample(clients, sample_sz)\n",
        "\n",
        "def random_FL(model, rd, lr, momentum, weight_decay, epoch):\n",
        "  clients = [_ for _ in range(100)]\n",
        "  for r in range(rd):\n",
        "    sampled = random_client_sampler(clients, 5)\n",
        "    loaders = []\n",
        "    for i in sampled:\n",
        "      loaders.append(DataLoader(data_train, batch_size=20, drop_last=True, sampler=sampler.SubsetRandomSampler(partitions[i])))\n",
        "    local_updates = {}\n",
        "    for i in range(5):\n",
        "      local_updates[sampled[i]] = train(model, loaders[i], lr, momentum, weight_decay, epoch)\n",
        "    m = average_model(local_updates)\n",
        "    acc, loss = test(m, test_loader, r+1)\n",
        "    print(\"Round {}, acc: {}%, loss: {}\".format(r+1, acc, loss))\n",
        "\n",
        "def bandit_FL(model, sample_sz, rd, lr, momentum, weight_decay, epoch, acc, batch_sz):\n",
        "  clients = [_ for _ in range(100)]\n",
        "  clients_info = {}\n",
        "  for i in clients:\n",
        "    clients_info[i] = ClientInfo(i)\n",
        "  explored = []\n",
        "  unexplored = clients\n",
        "  explore_ratio = 0.9\n",
        "  for cur_rd in range(1, rd+1):\n",
        "    sampled = select_participants(explored, unexplored, sample_sz, explore_ratio, cur_rd, clients_info)\n",
        "    print(sampled)\n",
        "    loaders = []\n",
        "    for i in sampled:\n",
        "      loaders.append(DataLoader(data_train, batch_size=batch_sz, drop_last=True, sampler=sampler.SubsetRandomSampler(partitions[i])))\n",
        "    local_updates = {}\n",
        "    for i in range(sample_sz):\n",
        "      local_updates[sampled[i]] = train(model, loaders[i], lr, momentum, weight_decay, epoch)\n",
        "    model = average_model(local_updates, local_updates.keys())\n",
        "    acc, loss = test(model, test_loader)\n",
        "    print(\"Round {}, acc: {}%, loss: {}\".format(cur_rd, acc*100, loss))\n",
        "    print(\"Calculate shapley value\")\n",
        "    values_dict = cal_shapley_values(test_loader, local_updates, acc, batch_sz)\n",
        "    contrib = {}\n",
        "    for participant in sampled:\n",
        "      clients_info[participant].last_involved_round = cur_rd\n",
        "      clients_info[participant].contribution = (clients_info[participant].contribution * clients_info[participant].times + values_dict[participant]) / (clients_info[participant].times + 1)\n",
        "      clients_info[participant].times += 1\n",
        "      contrib[participant] = clients_info[participant].contribution\n",
        "\n",
        "    for cid in contrib:\n",
        "      if cid in unexplored:\n",
        "        unexplored.remove(cid)\n",
        "        explored.append(cid)\n",
        "\n",
        "    iter_explored = copy.deepcopy(explored)\n",
        "    for cid in iter_explored:\n",
        "      if clients_info[cid].times >= 10:\n",
        "        unexplored.append(cid)\n",
        "        explored.remove(cid)\n",
        "        clients_info[cid].times = 0\n",
        "        clients_info[cid].last_involved_round = 0\n",
        "\n",
        "    explore_ratio *= 0.98\n",
        "    explore_ratio = max(explore_ratio, 0.2)\n",
        "\n",
        "rd = 10\n",
        "epoch = 10\n",
        "batch_size = 20\n",
        "lr = 1e-2\n",
        "momentum = 0.9\n",
        "weight_decay = 4e-5\n",
        "\n",
        "test_loader = DataLoader(data_test, batch_size, shuffle=False, pin_memory=True, drop_last=False)\n",
        "model = torchvision.models.shufflenet_v2_x2_0(num_classes=10)\n",
        "acc, _ = test(model, test_loader)\n",
        "print(\"initial acc: {}%\".format(acc*100))\n",
        "bandit_FL(model, 5, rd, lr, momentum, weight_decay, epoch, acc, 20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3bWQMFcwKEl",
        "outputId": "3371d461-8886-4e43-b34a-46cff61a015c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial acc: 9.999999046325684%\n",
            "[48, 21, 63, 43, 51]\n",
            "Round 1, acc: 26.219999313354492%, loss: 0.15444363471269607\n",
            "Calculate shapley value\n",
            "[99, 92, 14, 84, 48]\n",
            "Round 2, acc: 29.309999465942383%, loss: 0.17149744911193848\n",
            "Calculate shapley value\n"
          ]
        }
      ]
    }
  ]
}